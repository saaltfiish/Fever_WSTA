{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_model.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4COKqLFseAXy",
        "colab_type": "text"
      },
      "source": [
        "### **Set TPU environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pJkZcIld3dd",
        "colab_type": "code",
        "outputId": "1f0aa658-b939-47e7-83be-d587c505dcd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import datetime\n",
        "import json\n",
        "import os\n",
        "import pprint\n",
        "import random\n",
        "import string\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO_VJCjed7K9",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare and import BERT modules**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b94rUGkHhktH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaYyUq_8hkFn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "\n",
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "\n",
        "# import python modules defined by BERT\n",
        "import modeling\n",
        "import optimization\n",
        "import run_classifier\n",
        "import run_classifier_with_tfhub\n",
        "import tokenization\n",
        "\n",
        "# import tfhub \n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOsp_FIPhwaw",
        "colab_type": "text"
      },
      "source": [
        "**Add own processor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqw0qZ09hvTl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InputExample(object):\n",
        "  \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "  def __init__(self, guid, text_a, text_b=None, label=None, page_identifier=None):\n",
        "    \"\"\"Constructs a InputExample.\n",
        "    Args:\n",
        "      guid: Unique id for the example.\n",
        "      text_a: string. The untokenized text of the first sequence. For single\n",
        "        sequence tasks, only this sequence must be specified.\n",
        "      text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "        Only must be specified for sequence pair tasks.\n",
        "      label: (Optional) string. The label of the example. This should be\n",
        "        specified for train and dev examples, but not for test examples.\n",
        "      page_identifier: (Optional) string. The page identifier of the sentence.\n",
        "    \"\"\"\n",
        "    self.guid = guid\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b\n",
        "    self.label = label\n",
        "    self.page_identifier = page_identifier\n",
        "    \n",
        "\n",
        "class myProcessor():\n",
        "  def read_json(self, path, file_name):\n",
        "    \"\"\"Reads json file.\"\"\"\n",
        "    f_input = open(os.path.join(path, file_name), 'r')\n",
        "    content = json.load(f_input)\n",
        "    return content\n",
        "  \n",
        "  def get_train_examples(self, data_dir):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return self._create_examples(self.read_json(data_dir, \"train_origin_sentence6.json\"))\n",
        "\n",
        "  def get_dev_examples(self, data_dir):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return self._create_examples(self.read_json(data_dir, \"dev_origin_sentence.json\"))\n",
        "\n",
        "  def get_test_examples(self, data_dir):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return self._create_test_examples(self.read_json(data_dir, \"test.json\"))\n",
        "\n",
        "  def get_labels(self):\n",
        "    \"\"\"See base class.\"\"\"\n",
        "    return [\"NOT ENOUGH INFO\", \"SUPPORTS\", \"REFUTES\"]\n",
        "\n",
        "  def _create_examples(self, data_set):\n",
        "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "    examples = []\n",
        "    num = 0\n",
        "    for key in data_set:\n",
        "      text_a = tokenization.convert_to_unicode(data_set[key]['claim'])\n",
        "      label = tokenization.convert_to_unicode(data_set[key]['label'])\n",
        "      evidence_list = data_set[key]['evidence']\n",
        "      \n",
        "      for evidence in evidence_list:\n",
        "        num += 1\n",
        "        guid = key + '_' + str(evidence[1])\n",
        "        text_b = tokenization.convert_to_unicode(evidence[2])\n",
        "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label))\n",
        "\n",
        "    print(\"Total examples: %d\" % num)\n",
        "    print(\"Data set size: %d\" % len(data_set))\n",
        "    return examples\n",
        "  \n",
        "  def _create_test_examples(self, data_set):\n",
        "    \"\"\"Creates examples for the test sets.\"\"\"\n",
        "    examples = []\n",
        "    num = 0\n",
        "    for key in data_set:\n",
        "      text_a = tokenization.convert_to_unicode(data_set[key]['claim'])\n",
        "      label = tokenization.convert_to_unicode(data_set[key]['label'])\n",
        "      evidence_list = data_set[key]['evidence']\n",
        "      \n",
        "      for evidence in evidence_list:\n",
        "        num += 1\n",
        "        page_identifier = evidence[0]\n",
        "        guid = key + '_' + str(evidence[1])\n",
        "        text_b = tokenization.convert_to_unicode(evidence[2])\n",
        "        examples.append(InputExample(guid=guid, text_a=text_a, text_b=text_b, label=label, page_identifier=page_identifier))\n",
        " \n",
        "    print(\"Total examples: %d\" % num)\n",
        "    print(\"Data set size: %d\" % len(data_set))\n",
        "    return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXiTCCyqjkjS",
        "colab_type": "text"
      },
      "source": [
        "## **Prepare for training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7IzjT_oNjm6e",
        "colab_type": "code",
        "outputId": "9f1dfd51-09a0-455a-bd4d-b61e0c86d16e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TASK = 'web_search' #@param {type:\"string\"}\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "TASK_DATA_DIR = '/gdrive/My Drive/web_search'\n",
        "\n",
        "BUCKET = 'lcdmx' #@param {type:\"string\"}\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/bert-tfhub/models/{}'.format(BUCKET, TASK)\n",
        "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
        "\n",
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_MODEL_HUB = 'https://tfhub.dev/google/bert_' + BERT_MODEL + '/1'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_RtuyIDkc7i",
        "colab_type": "text"
      },
      "source": [
        "### **load tokenizer module from TF Hub**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01LXFnotkcJr",
        "colab_type": "code",
        "outputId": "699a8d70-f6f5-43fb-e12b-17f75146d13d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "tokenizer = run_classifier_with_tfhub.create_tokenizer_from_hub_module(BERT_MODEL_HUB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHdC7AD4lahc",
        "colab_type": "text"
      },
      "source": [
        "### **Prepare the training data and initialize TPU config**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GovDIyhOklxT",
        "colab_type": "code",
        "outputId": "72822c1b-8c41-4381-94db-1355c619f7de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "PREDICT_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Warmup is a period of time where the learning rate \n",
        "# is small and gradually increases--usually helps training.\n",
        "WARMUP_PROPORTION = 0.1\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000\n",
        "SAVE_SUMMARY_STEPS = 500\n",
        "\n",
        "processor = myProcessor()\n",
        "label_list = processor.get_labels()\n",
        "\n",
        "# Compute number of train and warmup steps from batch size\n",
        "train_examples = processor.get_train_examples(TASK_DATA_DIR)\n",
        "num_train_steps = int(len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "# Setup TPU related config\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "NUM_TPU_CORES = 8\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "\n",
        "def get_run_config(output_dir):\n",
        "  return tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=output_dir,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft1pmyp8z-jd",
        "colab_type": "text"
      },
      "source": [
        "## **Fine-tune and Run Predictions on a pretrained BERT Model from TF Hub**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmdUqGaG0Eaq",
        "colab_type": "code",
        "outputId": "bd5907ef-a331-4f23-f3d0-75e6fada2629",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        }
      },
      "source": [
        "# Force TF Hub writes to the GS bucket we provide.\n",
        "os.environ['TFHUB_CACHE_DIR'] = OUTPUT_DIR\n",
        "\n",
        "model_fn = run_classifier_with_tfhub.model_fn_builder(\n",
        "  num_labels=len(label_list),\n",
        "  learning_rate=LEARNING_RATE,\n",
        "  num_train_steps=num_train_steps,\n",
        "  num_warmup_steps=num_warmup_steps,\n",
        "  use_tpu=True,\n",
        "  bert_hub_module_handle=BERT_MODEL_HUB\n",
        ")\n",
        "\n",
        "estimator_from_tfhub = tf.contrib.tpu.TPUEstimator(\n",
        "  use_tpu=True,\n",
        "  model_fn=model_fn,\n",
        "  config=get_run_config(OUTPUT_DIR),\n",
        "  train_batch_size=TRAIN_BATCH_SIZE,\n",
        "  eval_batch_size=EVAL_BATCH_SIZE,\n",
        "  predict_batch_size=PREDICT_BATCH_SIZE,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9bCBvwL0L6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "def model_train(estimator):\n",
        "  print('Please wait...')\n",
        "  # We'll set sequences to be at most 128 tokens long.\n",
        "  train_features = run_classifier.convert_examples_to_features(\n",
        "      train_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(train_examples)))\n",
        "  print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "  tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "  train_input_fn = run_classifier.input_fn_builder(\n",
        "      features=train_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=True,\n",
        "      drop_remainder=True)\n",
        "  estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "  print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVgDmHgc0VQp",
        "colab_type": "code",
        "outputId": "b3ca5fe6-8063-42ab-a6ad-b34765d77b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11679
        }
      },
      "source": [
        "model_train(estimator_from_tfhub)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szfIa_Lo0azE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_eval(estimator):\n",
        "  # Eval the model.\n",
        "  eval_examples = processor.get_dev_examples(TASK_DATA_DIR)\n",
        "  eval_features = run_classifier.convert_examples_to_features(\n",
        "      eval_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  print('***** Started evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  print('  Num examples = {}'.format(len(eval_examples)))\n",
        "  print('  Batch size = {}'.format(EVAL_BATCH_SIZE))\n",
        "\n",
        "  # Eval will be slightly WRONG on the TPU because it will truncate\n",
        "  # the last batch.\n",
        "  eval_steps = int(len(eval_examples) / EVAL_BATCH_SIZE)\n",
        "  eval_input_fn = run_classifier.input_fn_builder(\n",
        "      features=eval_features,\n",
        "      seq_length=MAX_SEQ_LENGTH,\n",
        "      is_training=False,\n",
        "      drop_remainder=True)\n",
        "  result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n",
        "  print('***** Finished evaluation at {} *****'.format(datetime.datetime.now()))\n",
        "  output_eval_file = os.path.join(OUTPUT_DIR, \"eval_results.txt\")\n",
        "  with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
        "    print(\"***** Eval results *****\")\n",
        "    for key in sorted(result.keys()):\n",
        "      print('  {} = {}'.format(key, str(result[key])))\n",
        "      writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dLz16L30d-_",
        "colab_type": "code",
        "outputId": "32ff7f60-2a77-4815-82d5-1efbdfe535a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 10201
        }
      },
      "source": [
        "model_eval(estimator_from_tfhub)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lR0rHcRl0f4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def choose_label(evidence_list):\n",
        "  pre_label = [0, 0, 0]\n",
        "  for evidence in evidence_list:\n",
        "    if evidence[2] == 'NOT ENOUGH INFO':\n",
        "      pre_label[0] += 1\n",
        "    elif evidence[2] == 'SUPPORTS':\n",
        "      pre_label[1] += 1\n",
        "    elif evidence[2] == 'REFUTES':\n",
        "      pre_label[2] += 1\n",
        "    \n",
        "  majority = pre_label.index(max(pre_label))\n",
        "  if majority == 0:\n",
        "    if pre_label[0] == pre_label[1]:\n",
        "      majority = 1\n",
        "    elif pre_label[0] == pre_label[2]:\n",
        "      majority = 2\n",
        "    \n",
        "    if pre_label[1] != 0:\n",
        "      majority = 1\n",
        "    if pre_label[2] != 0:\n",
        "      majority = 2\n",
        "    \n",
        "  if majority == 0:\n",
        "    label = 'NOT ENOUGH INFO'\n",
        "  elif majority == 1:\n",
        "    label = 'SUPPORTS'\n",
        "  elif majority == 2:\n",
        "    label = 'REFUTES'\n",
        "  \n",
        "  new_evidence = []\n",
        "  if label == 'NOT ENOUGH INFO':\n",
        "    return label, []\n",
        "  elif label == 'SUPPORTS':\n",
        "    for evidence in evidence_list:\n",
        "      if evidence[2] == 'SUPPORTS':\n",
        "        new_evidence.append([evidence[0], int(evidence[1])])\n",
        "    return label, new_evidence\n",
        "  elif label == 'REFUTES':\n",
        "    for evidence in evidence_list:\n",
        "      if evidence[2] == 'REFUTES':\n",
        "        new_evidence.append([evidence[0], int(evidence[1])])\n",
        "    return label, new_evidence\n",
        "  \n",
        "def model_predict(estimator):\n",
        "  # Make predictions on a subset of eval examples\n",
        "  prediction_examples = processor.get_test_examples(TASK_DATA_DIR)\n",
        "  input_features = run_classifier.convert_examples_to_features(\n",
        "      prediction_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
        "  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=True)\n",
        "  predictions = estimator.predict(predict_input_fn)\n",
        "  \n",
        "  pre_list = {}\n",
        "  prev_key = \"\"\n",
        "  num = 0\n",
        "  evidence_list = []\n",
        "  for example, prediction in zip(prediction_examples, predictions):\n",
        "    key = example.guid.split('_')[0]\n",
        "    \n",
        "    if num == 0:\n",
        "      prev_key = key\n",
        "      \n",
        "    claim = example.text_a\n",
        "    pre_label = prediction['probabilities'].tolist()\n",
        "    pos = pre_label.index(max(pre_label))\n",
        "    \n",
        "    if pos == 0:\n",
        "      label = \"NOT ENOUGH INFO\"\n",
        "    elif pos == 1:\n",
        "      label = \"SUPPORTS\"\n",
        "    elif pos == 2:\n",
        "      label = \"REFUTES\"\n",
        "        \n",
        "    page_identifier = example.page_identifier\n",
        "    sentence_number = example.guid.split('_')[1]\n",
        "    \n",
        "    if key not in pre_list:\n",
        "      pre_list[key] = {}\n",
        "      \n",
        "    pre_list[key]['claim'] = claim\n",
        "    pre_list[key]['label'] = ''\n",
        "    \n",
        "    if prev_key != key:\n",
        "      pre_list[prev_key]['evidence'] = evidence_list\n",
        "      evidence_list = []\n",
        "      \n",
        "    evidence_list.append([page_identifier, sentence_number, label])\n",
        "    \n",
        "    prev_key = key\n",
        "    num += 1\n",
        "    \n",
        "    print('key: %s \\nclaim: %s \\nsentence: %s prob: %s \\nlabel: %s \\n ' % \n",
        "          (key, claim, example.text_b, prediction['probabilities'],label))\n",
        "  \n",
        "  pre_list[prev_key]['evidence'] = evidence_list\n",
        "  \n",
        "  print(len(pre_list))\n",
        "\n",
        "  final_pre_list = {}\n",
        "  num1 = 0\n",
        "  num2 = 0\n",
        "  num3 = 0\n",
        "  for key in pre_list:\n",
        "    final_pre_list[key] = {}\n",
        "    final_pre_list[key]['claim'] = pre_list[key]['claim']\n",
        "    final_pre_list[key]['label'], final_pre_list[key]['evidence'] = choose_label(pre_list[key]['evidence'])\n",
        "    if final_pre_list[key]['label'] == 'NOT ENOUGH INFO':\n",
        "      num1 +=1\n",
        "    elif final_pre_list[key]['label'] == 'SUPPORTS':\n",
        "      num2 += 1\n",
        "    elif final_pre_list[key]['label'] == 'REFUTES':\n",
        "      num3 +=1\n",
        "  \n",
        "  print(len(final_pre_list))\n",
        "  print('NOT ENOUGH INFO: %s' % num1)\n",
        "  print('SUPPORTS: %s' % num2)\n",
        "  print('REFUTES: %s' % num3)\n",
        "\n",
        "  f_output = open(os.path.join(TASK_DATA_DIR, \"testoutput.json\"), 'w')\n",
        "  json.dump(final_pre_list, f_output)\n",
        "  f_output.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRUVS_0Ra6m0",
        "colab_type": "code",
        "outputId": "13b8d84e-e98a-4531-889b-e09c37db3649",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2467369
        }
      },
      "source": [
        "model_predict(estimator_from_tfhub) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}